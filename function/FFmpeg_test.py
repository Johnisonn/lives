import aiohttp
import asyncio
from datetime import datetime
import subprocess
from concurrent.futures import ThreadPoolExecutor
import requests
from tqdm.asyncio import tqdm as async_tqdm
from tqdm import tqdm
from urllib.parse import urlparse
import json


# ç¬¬ä¸€é˜¶æ®µï¼šå¸¦è¿›åº¦æ¡çš„å¼‚æ­¥å¿«é€Ÿæ£€æµ‹
async def check_live_source(session, url, timeout=5):
    try:
        start = datetime.now()
        async with session.get(url, timeout=timeout) as resp:
            if resp.status == 200:
                await resp.content.read(1024)
                delay = (datetime.now() - start).total_seconds()
                return {"url": url, "delay": delay, "fast_check": True}
            return {"url": url, "delay": None, "fast_check": False}
    except Exception:
        return {"url": url, "delay": None, "fast_check": False}


async def fast_check(urls):
    async with aiohttp.ClientSession() as session:
        tasks = [check_live_source(session, url) for url in urls]
        results = []
        # ä½¿ç”¨å¼‚æ­¥è¿›åº¦æ¡
        for task in async_tqdm.as_completed(tasks, desc="ğŸš€ å¿«é€Ÿç­›é€‰", unit="ä¸ª"):
            results.append(await task)
        return results


# ç¬¬äºŒé˜¶æ®µï¼šå¸¦è¿›åº¦æ¡çš„FFmpegæ£€æµ‹
def ffmpeg_test(item, test_duration=10):
    command = [
        'ffmpeg',
        '-i', item['url'],
        '-t', str(test_duration),
        '-c', 'copy',
        '-f', 'null',
        '-loglevel', 'error',
        '-'
    ]
    try:
        result = subprocess.run(
            command,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            timeout=test_duration + 5
        )
        return {**item, 'ffmpeg_check': result.returncode == 0}
    except subprocess.TimeoutExpired:
        return {**item, 'ffmpeg_check': False}


# åŸŸåæå–å‡½æ•°
def extract_domain(url):
    try:
        parsed = urlparse(url)
        if parsed.scheme in ['rtmp', 'rtsp']:
            # å¤„ç†ç‰¹æ®Šæµåª’ä½“åè®®æ ¼å¼
            netloc = parsed.netloc.split('/')[0]
            return netloc.split(':')[0]
        return parsed.hostname.split(':')[0] if parsed.hostname else None
    except:
        return None


# ä¸»æ£€æµ‹æµç¨‹
async def main(urls, test_duration=10):
    # å¿«é€Ÿç­›é€‰
    fast_results = await fast_check(urls)
    valid_sources = [res for res in fast_results if res['fast_check']]

    # æ·±åº¦æ£€æµ‹
    ffmpeg_results = []
    with ThreadPoolExecutor() as executor:
        # ä½¿ç”¨çº¿ç¨‹æ± +è¿›åº¦æ¡
        tasks = [item for item in valid_sources]
        with tqdm(total=len(tasks), desc="ğŸ” æ·±åº¦æ£€æµ‹", unit="ä¸ª") as pbar:
            for result in executor.map(lambda x: ffmpeg_test(x, test_duration), tasks):
                ffmpeg_results.append(result)
                pbar.update(1)

    # åˆå¹¶æœ€ç»ˆç»“æœ
    final_results = []
    domain_whitelist = set()

    for result in fast_results:
        ffmpeg_res = next(
            (x for x in ffmpeg_results if x['url'] == result['url']),
            {'ffmpeg_check': False}
        )
        final_item = {
            **result,
            'ffmpeg_check': ffmpeg_res['ffmpeg_check']
        }
        final_results.append(final_item)

        # æ”¶é›†ç™½åå•åŸŸå
        if final_item['ffmpeg_check']:
            domain = extract_domain(final_item['url'])
            if domain:
                domain_whitelist.add(domain)

    # ä¿å­˜ç™½åå•
    with open('whitelist.txt', 'w') as f:
        f.write("\n".join(sorted(domain_whitelist)))

    return final_results


# æ”¹è¿›çš„ç»“æœå±•ç¤º
def print_results(results):
    print("\nğŸ“Š æ£€æµ‹ç»“æœæ±‡æ€»ï¼š")
    headers = ["URL", "å»¶è¿Ÿ", "å¿«é€Ÿ", "æµç•…", "åŸŸå"]
    row_format = "{:<40} | {:<6} | {:<4} | {:<4} | {:<20}"
    print(row_format.format(*headers))
    print("-" * 85)

    for res in results:
        domain = extract_domain(res['url']) or "N/A"
        print(row_format.format(
            res['url'][:40],
            f"{res['delay']:.2f}s" if res['delay'] else "è¶…æ—¶",
            'âœ“' if res['fast_check'] else 'âœ—',
            'âœ“' if res['ffmpeg_check'] else 'âœ—',
            domain[:20]
        ))


if __name__ == "__main__":

    resp = requests.get('https://github.moeyy.xyz/https://raw.githubusercontent.com/Johnisonn/lives/main/live.txt')
    resp.encoding = 'utf-8'
    lines = resp.text.split('\n')
    '''
    with open('/home/uos/Desktop/chs.txt', 'r') as file:
        lines = file.readlines()
    '''
    test_urls = []
    for line in lines:
        if 'æ²³åŒ—å«è§†' in line:
            line = line.strip()
            line = line.split(',')[1]
            line = line.split('$')[0]
            test_urls.append(line)


    final_results = asyncio.run(main(test_urls))
    print_results(final_results)

    # æ˜¾ç¤ºç™½åå•
    with open('whitelist.txt') as f:
        print("\nğŸ‰ åŸŸåç™½åå•ï¼š")
        print(f.read())

# ä¾èµ–å®‰è£…ï¼š
# pip install aiohttp tqdm